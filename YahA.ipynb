{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import ngrams_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '6']\n",
      "[[\"why doesn't an optical mouse work on a glass table?\", 'or even on some surfaces?', 'Optical mice use an LED and a camera to rapidly capture images of the surface beneath the mouse.  The infomation from the camera is analyzed by a DSP (Digital Signal Processor) and used to detect imperfections in the underlying surface and determine motion. Some materials, such as glass, mirrors or other very shiny, uniform surfaces interfere with the ability of the DSP to accurately analyze the surface beneath the mouse.  \\\\nSince glass is transparent and very uniform, the mouse is unable to pick up enough imperfections in the underlying surface to determine motion.  Mirrored surfaces are also a problem, since they constantly reflect back the same image, causing the DSP not to recognize motion properly. When the system is unable to see surface changes associated with movement, the mouse will not work properly.'], ['What is the best off-road motorcycle trail ?', 'long-distance trail throughout CA', 'i hear that the mojave road is amazing!<br />\\\\nsearch for it online.']]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "with open('yahoo_answers_csv/train.csv', 'r', encoding='UTF8') as fr:\n",
    "    reader = csv.reader(fr)\n",
    "    for line in reader:\n",
    "        labels.append(line[0])\n",
    "        texts.append(line[1:])\n",
    "print(labels[:2])\n",
    "print(texts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"why doesn't an optical mouse work on a glass table? or even on some surfaces? Optical mice use an LED and a camera to rapidly capture images of the surface beneath the mouse.  The infomation from the camera is analyzed by a DSP (Digital Signal Processor) and used to detect imperfections in the underlying surface and determine motion. Some materials, such as glass, mirrors or other very shiny, uniform surfaces interfere with the ability of the DSP to accurately analyze the surface beneath the mouse.  \\\\nSince glass is transparent and very uniform, the mouse is unable to pick up enough imperfections in the underlying surface to determine motion.  Mirrored surfaces are also a problem, since they constantly reflect back the same image, causing the DSP not to recognize motion properly. When the system is unable to see surface changes associated with movement, the mouse will not work properly.\",\n",
       " 'What is the best off-road motorcycle trail ? long-distance trail throughout CA i hear that the mojave road is amazing!<br />\\\\nsearch for it online.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = []\n",
    "for line in texts:\n",
    "    temp = line[0] +' ' + line[1] + ' ' + line[2]\n",
    "    text.append(temp)\n",
    "\n",
    "text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data):\n",
    "    for line in data:\n",
    "        tokens = tokenizer(line)\n",
    "        yield list(ngrams_iterator(tokens, 2))\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(text), min_freq=3, specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "text_pipeline = lambda x: vocab(list(ngrams_iterator(tokenizer(x), 2)))\n",
    "label_pipeline = lambda x: int(x) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[203, 11, 63, 583]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['here', 'is', 'an', 'example'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))  # '1', '2', '3', '4' -> [0, 1, 2, 3]\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)  # [475, 21, 30, 5297]\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)  # input의 누적 합계를 반환\n",
    "    text_list = torch.cat(text_list)  # batch 내의 모든 단어가 일렬로 들어감 -> nn.Embedding 에 들어가기 위해 하나로 합쳐짐\n",
    "\n",
    "    return label_list, text_list, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_class, dropout_p):\n",
    "        super(FastText, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_size, sparse=True)\n",
    "        nn.init.normal_(self.embedding.weight, mean=0.0, std=0.5)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.linear = nn.Linear(embedding_size, num_class, bias=True)\n",
    "        nn.init.normal_(self.linear.weight, mean=0.0, std=0.5)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        embedded = self.dropout(embedded)\n",
    "        return self.linear(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "log_interval = 3000\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, scheduler, clip):\n",
    "    model.train()\n",
    "    acc, count = 0, 0\n",
    "    s_time = time.time()\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)  # |predicted_label| = (batch, num_classes)\n",
    "        loss = criterion(predicted_label, label)\n",
    "\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), clip, norm_type=2)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        acc += (predicted_label.argmax(1) == label).sum().item()  # 같으면 1 -> 쭉 더함 \n",
    "        count += label.size(0)  # batch 때문에 size(0)으로 카운트 셈\n",
    "\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elasped = (time.time() - s_time)\n",
    "            print('accuracy: {}, time: {}[s]'.format(acc/count, int(elasped)))\n",
    "            s_time = time.time()   \n",
    "    return acc/count\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    v_total_acc, v_total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (v_label, v_text, v_offsets) in dataloader:\n",
    "            v_predicted_label = model(v_text, v_offsets)\n",
    "            v_total_acc += (v_predicted_label.argmax(1) == v_label).sum().item()\n",
    "            v_total_count += v_label.size(0)\n",
    "\n",
    "    return v_total_acc/v_total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# train_iter = AmazonReviewFull(split='train')\n",
    "num_class = len(set([label for label in labels]))\n",
    "print(num_class)\n",
    "\n",
    "save_dir = './saved_model/YahA'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "save_path = os.path.join(save_dir, 'ckpt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9', '2']\n",
      "[\"What makes friendship click? How does the spark keep going? good communication is what does it.  Can you move beyond small talk and say what's really on your mind.  If you start doing this, my expereince is that potentially good friends will respond or shun you.  Then you know who the really good friends are.\", 'Why does Zebras have stripes? What is the purpose or those stripes? Who do they serve the Zebras in the wild life? this provides camouflage - predator vision is such that it is usually difficult for them to see complex patterns']\n"
     ]
    }
   ],
   "source": [
    "test_labels = []\n",
    "test_text = []\n",
    "with open('yahoo_answers_csv/test.csv', 'r', encoding='UTF8') as fr:\n",
    "    reader = csv.reader(fr)\n",
    "    for line in reader:\n",
    "        test_labels.append(line[0])\n",
    "        temp = line[1] +' ' + line[2] + ' ' + line[3]\n",
    "        test_text.append(temp)\n",
    "\n",
    "print(test_labels[:2])\n",
    "print(test_text[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = []\n",
    "for i in range(len(labels)):\n",
    "    temp = []\n",
    "    temp.append(labels[i])\n",
    "    temp.append(text[i])\n",
    "    train_dataset.append(temp)\n",
    "\n",
    "test_dataset = []\n",
    "for i in range(len(test_labels)):\n",
    "    temp = []\n",
    "    temp.append(test_labels[i])\n",
    "    temp.append(test_text[i])\n",
    "    test_dataset.append(temp)\n",
    "    \n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['9',\n",
       "  \"What makes friendship click? How does the spark keep going? good communication is what does it.  Can you move beyond small talk and say what's really on your mind.  If you start doing this, my expereince is that potentially good friends will respond or shun you.  Then you know who the really good friends are.\"],\n",
       " ['2',\n",
       "  'Why does Zebras have stripes? What is the purpose or those stripes? Who do they serve the Zebras in the wild life? this provides camouflage - predator vision is such that it is usually difficult for them to see complex patterns']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_size = 64\n",
    "dropout_p = 0.2\n",
    "model = FastText(vocab_size, embedding_size, num_class, dropout_p)\n",
    "max_epoch = 5\n",
    "lr = 0.1\n",
    "lr_decay = 0.99\n",
    "step_size = 1000\n",
    "batch_size = 64\n",
    "clip = 3\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=lr_decay)\n",
    "\n",
    "# train_iter, test_iter = AmazonReviewFull()\n",
    "# train_dataset = to_map_style_dataset(train_iter)\n",
    "# test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------epoch: 1/5\n",
      "accuracy: 0.6673400533155615, time: 151[s]\n",
      "accuracy: 0.6860653432761207, time: 150[s]\n",
      "accuracy: 0.6946190006665925, time: 151[s]\n",
      "accuracy: 0.700048433463878, time: 149[s]\n",
      "accuracy: 0.7037926638224118, time: 152[s]\n",
      "accuracy: 0.70671160629965, time: 151[s]\n",
      "-----------------------------------------------------------\n",
      "epoch   1 | time: 1059.75s | valid accuracy:    0.726 \n",
      "-----------------------------------------------------------\n",
      "----------epoch: 2/5\n",
      "accuracy: 0.788252874041986, time: 151[s]\n",
      "accuracy: 0.7894908140309949, time: 151[s]\n",
      "accuracy: 0.7897733585157205, time: 151[s]\n",
      "accuracy: 0.7901750374968752, time: 150[s]\n",
      "accuracy: 0.7906962452503167, time: 152[s]\n",
      "accuracy: 0.7909603980334425, time: 152[s]\n",
      "-----------------------------------------------------------\n",
      "epoch   2 | time: 1062.13s | valid accuracy:    0.728 \n",
      "-----------------------------------------------------------\n",
      "----------epoch: 3/5\n",
      "accuracy: 0.8299629290236588, time: 151[s]\n",
      "accuracy: 0.8305777162139644, time: 149[s]\n",
      "accuracy: 0.8304355071658704, time: 149[s]\n",
      "accuracy: 0.8306820785767852, time: 149[s]\n",
      "accuracy: 0.8310518882074528, time: 150[s]\n",
      "accuracy: 0.831465960224432, time: 150[s]\n",
      "----------epoch: 4/5\n",
      "accuracy: 0.860817227590803, time: 151[s]\n",
      "accuracy: 0.8612627062156307, time: 150[s]\n",
      "accuracy: 0.8611005027219197, time: 150[s]\n",
      "accuracy: 0.8613995708690942, time: 150[s]\n",
      "accuracy: 0.861577978134791, time: 150[s]\n",
      "accuracy: 0.8614052691517138, time: 155[s]\n",
      "----------epoch: 5/5\n",
      "accuracy: 0.883642952349217, time: 150[s]\n",
      "accuracy: 0.8826966338943509, time: 150[s]\n",
      "accuracy: 0.8824470892123097, time: 151[s]\n",
      "accuracy: 0.8826608199316723, time: 151[s]\n",
      "accuracy: 0.8827953136457569, time: 150[s]\n",
      "accuracy: 0.8829518151769347, time: 151[s]\n"
     ]
    }
   ],
   "source": [
    "def run_train():\n",
    "  best_accu = 0\n",
    "\n",
    "  for epoch in range(0, max_epoch):\n",
    "    print('-' * 10 + 'epoch: {}/{}'.format(epoch+1, max_epoch))\n",
    "    epoch_start_time = time.time()\n",
    "    total_acc = train(model, train_dataloader, criterion, optimizer, scheduler, clip)\n",
    "    accu_val = evaluate(model, valid_dataloader)\n",
    "    if best_accu < accu_val:\n",
    "      best_accu = accu_val\n",
    "      torch.save({'epoch': epoch + 1, 'model_state_dict': model.state_dict()}, save_path)\n",
    "      print('-' * 59)\n",
    "      print('epoch {:3d} | time: {:5.2f}s | valid accuracy: {:8.3f} '.format(epoch+1, time.time() - epoch_start_time, accu_val))\n",
    "      print('-' * 59)\n",
    "\n",
    "run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7234166666666667\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate(model, test_dataloader)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2\n",
      "Accuracy:  0.7300833333333333\n"
     ]
    }
   ],
   "source": [
    "load_path = save_dir + '/ckpt.pth'\n",
    "checkpoint = torch.load(load_path)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "print('epoch: ', epoch)\n",
    "accuracy = evaluate(model, test_dataloader)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef7f9a8012d9131766e31894c279374cc63c73121ed4db3b9e67a294a4bf0e74"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
